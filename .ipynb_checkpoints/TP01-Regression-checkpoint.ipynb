{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TP 01 : Régression linéaire et logistique binaire\n",
    "\n",
    "Binômes : \n",
    "- Serbouh Celine\n",
    "- Benamar Akram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Réalisation des algorithmes\n",
    "\n",
    "Cette partie sert à améliorer la compréhension les algorithmes d'apprentissage automatique vus en cours en les implémentant à partir de zéro. \n",
    "Pour ce faire, on va utiliser la bibliothèque **numpy** qui est utile dans les calcules surtout matricielles.\n",
    "\n",
    "### I.1. Regression linéaire \n",
    "\n",
    "Premièrement, on va implémenter les fonctions nécéssaires pour la régression linéaire.\n",
    "\n",
    "#### I.1.1. Fonction de prédiction\n",
    "\n",
    "$$Z = zf(X, theta) = \\theta_0 + \\sum\\limits_{j=1}^{N} \\theta_j X_j = \\sum\\limits_{j=0}^{N} \\theta_j X_j | X_0 = 1 $$\n",
    "\n",
    "- X : une matrice de M lignes (échantillons) et (N + 1) colonnes (caractéristiques). Avant d'utiliser cette fonction, on doit augmenter la matrice originale avec une colonne pour $X_0 = 1$ si on veut utiliser $\\theta_0$\n",
    "- $\\theta$ : un vecteur des paramétres avec une taille (N + 1)\n",
    "- Z : un vecteur de taille M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1. , 1.7, 2.4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Calculer la combinaison linéaire\n",
    "def zf(X, theta):\n",
    "    a=X@theta #produit matriciel\n",
    "    return a\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : array([1. , 1.7, 2.4])\n",
    "#---------------------------------------------------------------------\n",
    "X_t = np.array([[1., 100.], [1., 200.], [1., 300.]])\n",
    "Theta_t = np.array([0.3, 0.007])\n",
    "zf(X_t, Theta_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.1.2. Fonction du coût\n",
    "\n",
    "La fonction du coût calcule l'erreur entre les valeurs estimées ($H$) en utilisant la fonction de prédiction $z$ précédente (H = Z(X, theta) et les valeurs réelles ($Y$).\n",
    "Ici, on va utiliser l'erreur quadratique moyenne (MSE: mean square error). \n",
    "\n",
    "$$J = MSE(Y, H) = \\frac{1}{2M} \\sum\\limits_{i=1}^{M} (Y - H)^2$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016666666666666673"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Définir la fonction du coût MSE\n",
    "def MSE(Y, H):\n",
    "    leng=len(Y)\n",
    "    sum=0\n",
    "    for i in range(leng):\n",
    "        a=Y[i]\n",
    "        b=H[i]\n",
    "        sum=(a-b)**2+sum\n",
    "    J=sum/(2*leng)\n",
    "    return J\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : 0.016666666666666673\n",
    "#---------------------------------------------------------------------\n",
    "H_t = np.array([1. , 1.7, 2.4])\n",
    "Y_t = np.array([1., 2., 2.5])\n",
    "MSE(Y_t, H_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le gradient de cette fonction est calculé comme suit : \n",
    "\n",
    "$$\\frac{\\partial MSE}{\\partial \\theta_j} \n",
    "= \\frac{\\partial }{\\partial \\theta_j} \\frac{1}{2M} \\sum\\limits_{i=1}^{M} (Y^{(i)} - H^{(i)})^2 \n",
    "= \\frac{1}{2M} \\sum\\limits_{i=1}^{M} [\\frac{\\partial }{\\partial \\theta_j} (Y^{(i)} - H^{(i)})^2]\n",
    "= \\frac{1}{2M} \\sum\\limits_{i=1}^{M} [-2 (Y^{(i)} - H^{(i)}) \\frac{\\partial }{\\partial \\theta_j} H^{(i)}]\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial MSE}{\\partial \\theta_j}\n",
    "= \\frac{1}{M} \\sum\\limits_{i=1}^{M} [(H^{(i)} - Y^{(i)}) \\frac{\\partial }{\\partial \\theta_j} \\sum\\limits_{k=0}^{N} \\theta_k X_k^{(i)}]\n",
    "= \\frac{1}{M} \\sum\\limits_{i=1}^{M} [(H^{(i)} - Y^{(i)}) \\frac{\\partial }{\\partial \\theta_j} \\theta_j X_j^{(i)}]\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial MSE}{\\partial \\theta_j} = \\frac{1}{M} \\sum\\limits_{i=1}^{M} (H^{(i)} - Y^{(i)}) X_j^{(i)}$$\n",
    "\n",
    "- \n",
    "Où $X^{(i)}$ est l'échantillon $i$ et $Y^{(i)}$ est la prédiction. $X_0^{(i)} = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -0.13333333, -30.        ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Définir la fonction de gradient MSE\n",
    "# Elle doit retourner les gradients de tous les thétas\n",
    "def dMSE(X, Y, H):\n",
    "   # gradient #le tableau qui va contenir les gradient\n",
    "    sum=0\n",
    "    leng=len(X)\n",
    "    for i in range(leng):\n",
    "        a=H[i]\n",
    "        b=Y[i]\n",
    "        c=X[i]\n",
    "        sum=sum+(a-b)*c\n",
    "    theta=sum/leng\n",
    "    return theta\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : array([ -0.13333333, -30.        ])\n",
    "#---------------------------------------------------------------------\n",
    "X_t = np.array([[1., 100.], [1., 200.], [1., 300.]])\n",
    "H_t = np.array([1. , 1.7, 2.4])\n",
    "Y_t = np.array([1., 2., 2.5])\n",
    "dMSE(X_t, Y_t, H_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2. Régression logistique\n",
    "\n",
    "Ici, on va implémenter la régression logistique qui se base sur la combinaison linéaire de la régression linaire.\n",
    "\n",
    "#### I.2.1. Fonction de prédiction\n",
    "\n",
    "$$P(X, theta) = Sg(zf(X, theta)) $$\n",
    "\n",
    "$$Sg(Z) = \\frac{1}{1+e^{-Z}}$$\n",
    "\n",
    "- X : une matrice de M lignes (échantillons) et (N + 1) colonnes (caractéristiques). Avant d'utiliser cette fonction, on doit augmenter la matrice originale avec une colonne pour $X_0 = 1$ si on veut utiliser $\\theta_0$\n",
    "- $\\theta$ : un vecteur des paramétres avec une taille (N + 1)\n",
    "- Z : un vecteur "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2689414213699951, 0.7310585786300049]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO La fonction ségmoid\n",
    "# Elle doit fonctionner sur des scalaires ou des vecteurs de numpy\n",
    "def sg(Z): \n",
    "    leng=len(Z)\n",
    "    sig=[]\n",
    "    print(leng)\n",
    "    for i in range(leng):\n",
    "        sig.append(1/(1+np.exp(-(Z[i]))))\n",
    "    return sig\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : (0.5, array([0.26894142, 0.73105858]))\n",
    "#---------------------------------------------------------------------\n",
    "#sg(0), \n",
    "sg(np.array([-1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5621765008857981, 0.9947798743064417, 0.022977369910025615]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Prédiction de probabilité dans la régression logistique\n",
    "# Probabilité de la régression logistique\n",
    "def Prl(X, Theta): \n",
    "    proba =[]\n",
    "    c =zf(X,Theta)\n",
    "    return sg(c)\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : array([0.5621765 , 0.99477987, 0.02297737])\n",
    "#---------------------------------------------------------------------\n",
    "X_t = np.array([[1., 0], [1., 10.], [1., -8]])\n",
    "Theta_t = np.array([0.25, 0.5])\n",
    "\n",
    "Prl(X_t, Theta_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.2. Fonction du coût\n",
    "\n",
    "La fonction du coût calcule l'erreur entre les valeurs estimées ($H$).\n",
    "Ici, on va utiliser l'entropie croisée binaire (BCE: Binary Cross Entropy Loss). \n",
    "\n",
    "$$BCE(Y^{(i)}, H^{(i)})\n",
    "= \\begin{cases}\n",
    "- \\log(H^{(i)}) & \\text{ si } Y^{(i)} = 1\\\\ \n",
    "- \\log(1 - H^{(i)})  & \\text{ si } Y^{(i)} = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "Puisque $Y^{(i)} \\in \\{0, 1\\}$, donc : \n",
    "\n",
    "$$ BCE(Y^{(i)}, H^{(i)}) = - Y^{(i)} \\log(H^{(i)}) - (1- Y^{(i)}) \\log(1 - H^{(i)}) $$\n",
    "\n",
    "$$J = BCE(Y, H) = \\frac{-1}{M} \\sum\\limits_{i=1}^{M} [Y^{(i)} \\log(H^{(i)}) + (1- Y^{(i)}) \\log(1 - H^{(i)})]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6181210324473608"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO : La fonction de cout BCE \n",
    "def BCE(Y, H):\n",
    "    from math import log\n",
    "    leng=len(Y)\n",
    "    sum=0\n",
    "    for i in range(leng):\n",
    "        a=Y[i]\n",
    "        b=H[i]\n",
    "        sum=sum+(a*log(b)+((1-a)*log(1-b)))\n",
    "    val=(-sum)/leng\n",
    "    return val \n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : 0.6181210324473608\n",
    "#---------------------------------------------------------------------\n",
    "Y_t = np.array([1., 0., 0., 1.])\n",
    "H_t = np.array([0.25, 0.25, 0.5, 0.9])\n",
    "BCE(Y_t, H_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le gradient de cette fonction est calculé comme suit : \n",
    "\n",
    "$$\\frac{\\partial BCE}{\\partial \\theta_j} \n",
    "= \\frac{-1}{M} \\sum\\limits_{i=1}^{M} \\frac{\\partial}{\\partial \\theta_j} [Y^{(i)} \\log(H^{(i)}) + (1- Y^{(i)}) \\log(1 - H^{(i)})]\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial BCE}{\\partial \\theta_j} \n",
    "= \\frac{-1}{M} \\sum\\limits_{i=1}^{M} [ Y^{(i)} \\frac{\\partial}{\\partial \\theta_j} \\log(H^{(i)}) + (1- Y^{(i)}) \\frac{\\partial}{\\partial \\theta_j}\\log(1 - H^{(i)})]\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial BCE}{\\partial \\theta_j} \n",
    "= \\frac{-1}{M} \\sum\\limits_{i=1}^{M} [ Y^{(i)} \\frac{1}{H^{(i)}} \\frac{\\partial}{\\partial \\theta_j} H^{(i)} + (1- Y^{(i)}) \\frac{-1}{1-H^{(i)}} \\frac{\\partial}{\\partial \\theta_j} H^{(i)})]\n",
    "= \\frac{-1}{M} \\sum\\limits_{i=1}^{M} \\frac{Y^{(i)}-H^{(i)}}{H^{(i)}(1-H^{(i)})} \\frac{\\partial}{\\partial \\theta_j} H^{(i)}\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial H^{(i)}}{\\partial \\theta_j} \n",
    "= \\frac{\\partial \\sigma(Z^{(i)})}{\\partial Z^{(i)}} \\frac{\\partial Z^{(i)}}{\\partial \\theta_j} \n",
    "= [\\sigma(Z^{(i)}) (1-\\sigma(Z^{(i)}))]\\frac{\\partial}{\\partial \\theta_j} \\sum\\limits_{k=0}^{N} \\theta_k X_k^{(i)}  \n",
    "= H^{(i)} (1-H^{(i)})  X_j^{(i)}\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial BCE}{\\partial \\theta_j} \n",
    "= \\frac{-1}{M} \\sum\\limits_{i=1}^{M} \\frac{Y^{(i)}-H^{(i)}}{H^{(i)}(1-H^{(i)})} [H^{(i)} (1-H^{(i)}) X_j^{(i)}]\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial BCE}{\\partial \\theta_j} = \\frac{1}{M} \\sum\\limits_{i=1}^{M} (H^{(i)} - Y^{(i)}) X_j^{(i)}$$\n",
    "\n",
    "- \n",
    "Où $X^{(i)}$ est l'échantillon $i$ et $Y^{(i)}$ est la prédiction. $X_0^{(i)} = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.08333333, -0.25      ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Définir la fonction de gradient BCE\n",
    "def dBCE(X, Y, H): \n",
    "    sum=0\n",
    "    leng=len(X)\n",
    "    for i in range(leng):\n",
    "        a=H[i]\n",
    "        b=Y[i]\n",
    "        c=X[i]\n",
    "        sum=sum+(a-b)*c\n",
    "    theta=sum/leng\n",
    "    return theta\n",
    "    \n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : array([0.08333333, 0.25      ])\n",
    "#---------------------------------------------------------------------\n",
    "X_t = np.array([[1., 5.], [1., 10.], [1., 8]])\n",
    "H_t = np.array([2.75, 5.25, 4.25])\n",
    "Y_t = np.array([3., 5., 4.5])\n",
    "dMSE(X_t, Y_t, H_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.3. Déscente du gradient \n",
    "\n",
    "\n",
    "#### I.3.1. Fonction de convergence\n",
    "\n",
    "Ici, on veut implémenter une fonction qui décide l'arrêt de la déscente du gradient en se basant sur : \n",
    "- L'erreur actuelle $J$ et l'erreur précédente $J0$\n",
    "- Nombre des itérations max $IT$ et l'itération actuelle $it$\n",
    "\n",
    "Cette fonction renvoit **True** si :\n",
    "- on a atteint le nombre maximal des itérations\n",
    "- l'erreur a augmenté (si var=True) sinon on ne prend pas la variation de l'erreur en considération"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True, True, False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Définir la fonction d'arret de la descente du gradient\n",
    "def arreter(J0, J, it_max, it, var=True):\n",
    "    arreter=False\n",
    "    if it == it_max :\n",
    "        arreter=True\n",
    "    else:\n",
    "        if var == True :\n",
    "            if J>J0:\n",
    "                arreter=True\n",
    "            else :\n",
    "                arreter=False\n",
    "        else:\n",
    "            arreter=False\n",
    "    return arreter\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : (False, True, True, False)\n",
    "#---------------------------------------------------------------------\n",
    "arreter(0.5, 0.2, 100, 0), arreter(0.1, 0.2, 100, 5), arreter(0.5, 0.2, 100, 100), arreter(0.1, 0.2, 100, 5, var=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.3.2. Fonction de mise à jour des paramètres\n",
    "\n",
    "$$\\theta_i = \\theta_i - \\alpha \\frac{\\partial J}{\\partial \\theta_j}$$\n",
    "\n",
    "Cette fonction doit retourner les nouveaux thetats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.30013333333, 0.037]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Définir la fonction de la mise à jours des paramètres\n",
    "def majTheta(Theta, Gradient, alpha): \n",
    "    thetas=[]\n",
    "    leng=len(Theta)\n",
    "    val=0\n",
    "    for i in range(leng):\n",
    "        thet=Theta[i]\n",
    "        grad=Gradient[i]\n",
    "        thetas.append(thet-alpha*grad)\n",
    "    return thetas\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : array([0.30013333, 0.037     ])\n",
    "#---------------------------------------------------------------------\n",
    "Theta_t = np.array([0.3, 0.007])\n",
    "Gradient_t = np.array([ -0.13333333, -30.])\n",
    "alpha_t = 0.001\n",
    "majTheta(Theta_t, Gradient_t, alpha_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.3.3. Déscente du gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.3  , 0.007]), [0.016666666666666653], 20.117448897777773)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def descente(X, Y, Theta, Hf=zf, Jf=MSE, dJf=dMSE, it_max=100, alpha=0.1, var=True): \n",
    "\n",
    "    H0 = Hf(X, Theta)\n",
    "    J0 = Jf(Y, H0)\n",
    "    it = 0\n",
    "    couts = []\n",
    "    \n",
    "    #pour ne pas modifier le contenu de Theta en entrée\n",
    "    Theta = Theta.copy()\n",
    "    \n",
    "    while True:\n",
    "        Theta_opt = Theta.copy()\n",
    "        couts.append(J0)\n",
    "    \n",
    "        Gradient = dJf(X, Y, H0)\n",
    "        Theta = majTheta(Theta, Gradient, alpha)\n",
    "        H = Hf(X, Theta)\n",
    "        J = Jf(Y, H)\n",
    "        \n",
    "        if arreter(J0, J, it_max, it, var=var):\n",
    "            break\n",
    "        \n",
    "        J0 = J\n",
    "        H0 = H\n",
    "        it += 1\n",
    "    \n",
    "    return Theta_opt, couts, J\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : (array([0.3  , 0.007]), [0.016666666666666653], 20.117448897777773)\n",
    "#---------------------------------------------------------------------\n",
    "X_t = np.array([[1., 100.], [1., 200.], [1., 300.]])\n",
    "Theta_t = np.array([0.3, 0.007])\n",
    "Y_t = np.array([1., 2., 2.5])\n",
    "IT_max = 5\n",
    "alpha_t = 0.001\n",
    "\n",
    "descente(X_t, Y_t, Theta_t, it_max=IT_max, alpha=alpha_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.4. Regroupement et test de l'implémentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import outils\n",
    "\n",
    "class Regression(object):\n",
    "    \n",
    "    def __init__(self, it_max=100, logistique=False, alpha=0.01, norm=True, const=True, var=True): \n",
    "        self.it_max = it_max\n",
    "        self.alpha = alpha\n",
    "        self.norm = norm\n",
    "        self.const = const\n",
    "        self.var = var\n",
    "        \n",
    "        if logistique :\n",
    "            self.Hf=Prl\n",
    "            self.Jf=BCE\n",
    "            self.dJf=dBCE\n",
    "        else:\n",
    "            self.Hf=zf\n",
    "            self.Jf=MSE\n",
    "            self.dJf=dMSE   \n",
    "    \n",
    "    def entrainer(self, X, Y): \n",
    "        X_pre, self.mean, self.std = outils.preparer(X, norm=self.norm, const=self.const)\n",
    "        Theta = outils.generer_aleatoire_1(X_pre.shape[1])\n",
    "        self.Theta, self.couts, self.dernier_cout = descente(X_pre, Y, Theta, \n",
    "                                                             Hf=self.Hf, Jf=self.Jf, dJf=self.dJf, \n",
    "                                                             it_max=self.it_max, alpha=self.alpha, var=self.var)\n",
    "        \n",
    "    # La prédiction\n",
    "    # si classes=False elle rend H sinon elle rend 0 ou 1 au cas de regression logistique\n",
    "    def predire(self, X, classes=False, seuil=0.5):\n",
    "        X_pre, self.mean, self.std = outils.preparer(X, norm=self.norm, const=self.const, mean=self.mean, std=self.std)\n",
    "        H = self.Hf(X_pre, self.Theta)\n",
    "        if classes and logistique:\n",
    "            return H >= seuil\n",
    "        return H\n",
    "\n",
    "#=====================================================================\n",
    "# TEST\n",
    "#=====================================================================\n",
    "\n",
    "reg_lin = Regression(norm=False)\n",
    "X_t = np.array([[5.], [10.], [8.]])\n",
    "Y_t = np.array([3., 5., 4.5])\n",
    "reg_lin.entrainer(X_t, Y_t)\n",
    "\n",
    "plt.scatter(X_t[:,0], Y_t, color=\"blue\")\n",
    "plt.plot(X_t[:,0], reg_lin.predire(X_t), color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Application et analyse\n",
    "\n",
    "Cette partie sert à appliquer les algorithmes, modifier les paramètres et analyser les résultats\n",
    "\n",
    "\n",
    "### II.1. Régression linéaire "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Superficie</th>\n",
       "      <th>Prix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2104</td>\n",
       "      <td>399900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1600</td>\n",
       "      <td>329900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2400</td>\n",
       "      <td>369000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1416</td>\n",
       "      <td>232000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3000</td>\n",
       "      <td>539900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Superficie    Prix\n",
       "0        2104  399900\n",
       "1        1600  329900\n",
       "2        2400  369000\n",
       "3        1416  232000\n",
       "4        3000  539900"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = [\"Superficie\", \"Prix\"]\n",
    "houses = pd.read_csv(\"datasets/houses.csv\", names=header)\n",
    "houses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "\n",
    "Xhouses = houses.iloc[:, :-1].values # Premières colonnes \n",
    "Yhouses = houses.iloc[:,-1].values # Dernière colonne \n",
    "Xhouses_train, Xhouses_test, Yhouses_train, Yhouses_test = train_test_split(Xhouses, Yhouses, test_size=0.2, random_state=0)  \n",
    "\n",
    "len(Xhouses_train), len(Xhouses_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.1.1 Taux d'apprentissage et convergence \n",
    "\n",
    "Ici, nous avons varié le taux d'apprentissage et afficher les courbes de la fonction objective par rapport à l'itération. \n",
    "\n",
    "**TODO : Analyser les résultats**\n",
    "- Que remarquez-vous ?\n",
    "- Donner une hypothèse\n",
    "- Essayer de justifier cette hypothèse par ce que vous avez vu en cours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "VAR = False\n",
    "NORM = True\n",
    "ITMAX = 500\n",
    "\n",
    "reg_lin1 = Regression(it_max=ITMAX, norm=NORM, alpha=1., var=VAR)\n",
    "reg_lin01 = Regression(it_max=ITMAX, norm=NORM, alpha=0.1, var=VAR)\n",
    "reg_lin001 = Regression(it_max=ITMAX, norm=NORM, alpha=0.01, var=VAR)\n",
    "reg_lin0001 = Regression(it_max=ITMAX, norm=NORM, alpha=0.001, var=VAR)\n",
    "\n",
    "reg_lin1.entrainer(Xhouses_train, Yhouses_train)\n",
    "reg_lin01.entrainer(Xhouses_train, Yhouses_train)\n",
    "reg_lin001.entrainer(Xhouses_train, Yhouses_train)\n",
    "reg_lin0001.entrainer(Xhouses_train, Yhouses_train)\n",
    "\n",
    "\n",
    "plt.plot(reg_lin1.couts, label = \"alpha=1\")\n",
    "plt.plot(reg_lin01.couts, label = \"alpha=0.1\")\n",
    "plt.plot(reg_lin001.couts, label = \"alpha=0.01\")\n",
    "plt.plot(reg_lin0001.couts, label = \"alpha=0.001\")\n",
    "plt.legend()\n",
    "#plt.autoscale()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-24-7a59eb9b9e41>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-24-7a59eb9b9e41>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    lorsque alpha=0.001 et nombre d'iter\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# 1-On remarque que la convergence de la fonction est plus rapide et meilleur lorsque le learning rate (alpha) croit  (est plus grand)\n",
    "lorsque alpha=0.001 et nombre d'iter\n",
    "    \n",
    "2-\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.1.2 Prediction \n",
    "\n",
    "**TODO : Analyser les résultats**\n",
    "- Que remarquez-vous ?\n",
    "- Est-ce que la régression linéaire peut prédire exactement les valeurs Y ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression  \n",
    "regressor1 = LinearRegression(normalize=True)  \n",
    "regressor1.fit(Xhouses_train, Yhouses_train) \n",
    "\n",
    "Yhouses_pred = regressor1.predict(Xhouses_test)  \n",
    "\n",
    "plt.scatter(Xhouses_train, Yhouses_train, color=\"blue\", label = \"entrainement\")\n",
    "plt.scatter(Xhouses_test, Yhouses_test, color=\"green\", label = \"test\")\n",
    "plt.plot(Xhouses_train, regressor1.predict(Xhouses_train), color=\"red\", label = \"line de prediction\")\n",
    "plt.scatter(Xhouses_test, Yhouses_pred, color=\"violet\", label = \"prediction\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.1.3 Prediction avec régression multinomiale\n",
    "\n",
    "Pour avoir des préductions sous formes des courbes non linéaires, nous avons générer des nouvelles caractéristiques : \n",
    "- $X^2$\n",
    "- $X^5$\n",
    "- $X^{10}$\n",
    "\n",
    "**TODO : Analyser les résultats**\n",
    "- Que remarquez-vous ?\n",
    "- Donner une hypothèse (en précisant si plus de données ou avoir des données complexes peut améliorer la performance)\n",
    "- Essayer de justifier cette hypothèse par ce que vous avez vu en cours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "degres = [2, 5, 10]\n",
    "erreurs = [mean_squared_error(Yhouses_test, Yhouses_pred)]\n",
    "couleurs=[\"green\", \"magenta\", \"blue\"]\n",
    "\n",
    "# Trouver les indexes ordonnées \n",
    "idx = Xhouses_train[:,0].argsort()\n",
    "\n",
    "plt.scatter(Xhouses_train[idx], Yhouses_train[idx], color=\"cyan\", label = \"entrainement\")\n",
    "plt.scatter(Xhouses_test, Yhouses_test, color=\"black\", label = \"test\")\n",
    "plt.plot(Xhouses_train, regressor1.predict(Xhouses_train), color=\"red\", label = \"line degre=1\")\n",
    "\n",
    "for i in range(len(degres)):\n",
    "    poly = PolynomialFeatures(degree=degres[i], include_bias=False)\n",
    "    Xhouse_train_poly = poly.fit_transform(Xhouses_train)\n",
    "    regresseur = LinearRegression(normalize=True)  \n",
    "    regresseur.fit(Xhouse_train_poly, Yhouses_train)  \n",
    "    plt.plot(Xhouses_train[idx], regresseur.predict(Xhouse_train_poly)[idx], color=couleurs[i], label = \"line degre=\" + str(degres[i]))\n",
    "    Xhouse_test_poly = poly.fit_transform(Xhouses_test)\n",
    "    Yhouses_pred = regresseur.predict(Xhouse_test_poly) \n",
    "    erreurs.append(mean_squared_error(Yhouses_test, Yhouses_pred))\n",
    "\n",
    "print(erreurs)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2. Régression logistique binaire "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = pd.read_csv(\"datasets/notes.csv\")\n",
    "\n",
    "# Extraction des features \n",
    "X_notes = notes.iloc[:, :-1].values # Premières colonnes \n",
    "\n",
    "Y_notes = notes.iloc[:,-1].values # Dernière colonne \n",
    "\n",
    "notes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.2.1 Normalisation et convergence \n",
    "\n",
    "Ici, on veut tester l'effet de la normalisation sur la convergence du modèle.\n",
    "\n",
    "**TODO : Analyser les résultats**\n",
    "- Que remarquez-vous ?\n",
    "- Donner une hypothèse\n",
    "- Essayer de justifier cette hypothèse par ce que vous avez vu en cours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAR = False\n",
    "ALPHA = 0.01\n",
    "ITMAX = 200\n",
    "\n",
    "reg_log_nonorm = Regression(it_max=ITMAX, norm=False, alpha=ALPHA, var=VAR, logistique=True)\n",
    "reg_log_nonorm.entrainer(X_notes, Y_notes)\n",
    "reg_log_norm = Regression(it_max=ITMAX, norm=True, alpha=ALPHA, var=VAR, logistique=True)\n",
    "reg_log_norm.entrainer(X_notes, Y_notes)\n",
    "\n",
    "plt.plot(reg_log_nonorm.couts, label = \"sans normalisation\")\n",
    "plt.plot(reg_log_norm.couts, label = \"avec normalisation\")\n",
    "\n",
    "plt.legend()\n",
    "#plt.autoscale()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.2.2 Complexité du modèle et apprentissage\n",
    "\n",
    "On veut améliorer la séparation entre les deux classes : \"admis\" et \"non admis\". Pour ce faire, on a proposé d'ajouter des caractéristiques complexes : $X^3$ et $X^5$.\n",
    "\n",
    "**TODO : Analyser les résultats**\n",
    "- Que remarquez-vous ? (les nouveaux modèles ont-il été capables d'améliorer le modèle initial ?)\n",
    "- Donner une hypothèse (Quelle est la raison ?) \n",
    "- Essayer de justifier cette hypothèse par ce que vous avez vu en cours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from outils import courbe_decision\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "degres = [3, 5]\n",
    "couleurs=[\"red\", \"magenta\", \"blue\"]\n",
    "\n",
    "def dessiner2D(modele, ax, xrange, yrange, poly=None, colors=[\"red\"], label=\"\"):\n",
    "    XX = [[[xr, yr] for xr in xrange] for yr in yrange]\n",
    "    XX = np.array(XX).reshape(-1, 2)\n",
    "    #grid = [[modele.predict(np.array([[xr, yr]])) for xr in xrange] for yr in yrange]\n",
    "    #grid = np.array(grid).reshape(len(xrange), len(yrange))\n",
    "    if poly:\n",
    "        XX = poly.fit_transform(XX)\n",
    "    grid = modele.predict(XX).reshape(len(xrange), len(yrange))\n",
    "    cs = ax.contour(xrange, yrange, grid, colors=colors,linewidths=(0.5),linestyles=('--'),levels=None)\n",
    "    cs.collections[1].set_label(label)\n",
    "\n",
    "\n",
    "X_notes_train = X_notes[:80,:]\n",
    "X_notes_test = X_notes[80:,:]\n",
    "Y_notes_train = Y_notes[:80]\n",
    "Y_notes_test = Y_notes[80:]\n",
    "# Trouver les indexes ordonnées \n",
    "idx2 = X_notes_train[:,0].argsort()\n",
    "\n",
    "oui_train = Y_notes_train == 1\n",
    "oui_test = Y_notes_test == 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "#ax.scatter(X_notes_train[:,0], X_notes_train[:,1], c=Y_notes_train, lw=0, alpha=1.)\n",
    "#ax.scatter(X_notes_test[:,0], X_notes_test[:,1], c=Y_notes_test, marker=\"x\", lw=0, alpha=1.)\n",
    "ax.scatter(X_notes_train[oui_train, 0], X_notes_train[oui_train, 1], color=\"green\", marker=\"o\", label=\"Admis (entrainement)\")\n",
    "ax.scatter(X_notes_train[~oui_train, 0], X_notes_train[~oui_train, 1], color=\"green\", marker=\"x\", label=\"Non Admis (entrainement)\")\n",
    "\n",
    "ax.scatter(X_notes_test[oui_test, 0], X_notes_test[oui_test, 1], color=\"red\", marker=\"o\", label=\"Admis (test)\")\n",
    "ax.scatter(X_notes_test[~oui_test, 0], X_notes_test[~oui_test, 1], color=\"red\", marker=\"x\", label=\"Non Admis (test)\")\n",
    "\n",
    "reglog1 = LogisticRegression(penalty='none')\n",
    "reglog1.fit(X_notes_train, Y_notes_train)\n",
    "\n",
    "xrange = np.linspace(0, 20, 50)\n",
    "yrange = np.linspace(0, 20, 50)\n",
    "dessiner2D(reglog1, ax, xrange, yrange, label=\"X\")\n",
    "\n",
    "for i in range(len(degres)):\n",
    "    poly = PolynomialFeatures(degree=degres[i], include_bias=False)\n",
    "    X_notes_train_poly = poly.fit_transform(X_notes_train)\n",
    "    reglogi = LogisticRegression(penalty='none')\n",
    "    reglogi.fit(X_notes_train_poly, Y_notes_train)\n",
    "    dessiner2D(reglogi, ax, xrange, yrange, poly=poly, colors=[couleurs[i+1]], label=\"X^\" + str(degres[i]))\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel(\"Note 1\")\n",
    "plt.ylabel(\"Note 2\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Good luck\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
